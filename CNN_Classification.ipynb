{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivianlin2000/Computer-Vision-Classification-for-Chest-X-Ray-Imaging/blob/main/CNN_Classification_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_84m8NatGpR0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from skimage import io, transform\n",
        "from keras.utils import to_categorical, load_img, img_to_array\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import keras\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ctHAEc9GpR5"
      },
      "source": [
        "# LOADING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjl0kFkIGpR6"
      },
      "outputs": [],
      "source": [
        "# load training data into dataframe\n",
        "def load_data(path):\n",
        "    path = Path(path)\n",
        "    normal = (path/'NORMAL').glob('*.jpeg')\n",
        "    pneumonia = (path/'PNEUMONIA').glob('*.jpeg')\n",
        "    data = [[img, 'NORMAL'] for img in normal] + [[img, 'PNEUMONIA'] for img in pneumonia]\n",
        "    df = pd.DataFrame(data, columns=['img', 'label'])\n",
        "    return df\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTJM-K_EGpR7"
      },
      "source": [
        "# DISPLAY DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebgDuTQ0GpR7"
      },
      "outputs": [],
      "source": [
        "# display x-ray images\n",
        "def display_image(data, preprocessed=False):\n",
        "    indices = np.arange(len(data.index))\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    plt.figure(figsize=(10,5))\n",
        "    for i in range(10):\n",
        "        ax = plt.subplot(2, 5, i+1)\n",
        "        img = data['img'][indices[i]]\n",
        "        if not preprocessed:\n",
        "            img = io.imread(str(img))\n",
        "            img = transform.resize(img, (224,224), anti_aliasing=True)\n",
        "        plt.imshow(img)\n",
        "        plt.title(data['label'][indices[i]])\n",
        "        plt.axis(\"off\")\n",
        "    plt.figure(figsize=(10,5))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1-GmzQvGpR8"
      },
      "source": [
        "# PREPROCESSING IMAGES\n",
        "- resize to all same size\n",
        "- 3 channel\n",
        "- RGB format\n",
        "- normalize to [0, 1]\n",
        "- convert labels to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbe4Xr9aGpR9"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df, as_gray = False):\n",
        "    # IMAGE SIZE = (224, 224)\n",
        "    df['img'] = df['img'].apply(lambda x: transform.resize(io.imread(str(x), as_gray=as_gray), (224,224), anti_aliasing=True))\n",
        "    \n",
        "    # 3 CHANNELS\n",
        "    df['img'] = df['img'].apply(lambda x: np.dstack([x, x, x]) if len(x.shape) == 2 else x)\n",
        "    \n",
        "    # NORMALIZE VALUES\n",
        "    # df['img'] = df['img'].apply(lambda x: x.astype(np.float32)/255.0)\n",
        "    \n",
        "    # CONVERT LABELS\n",
        "    # 0: NORMAL    1: PNEUMONIA\n",
        "    label_dict = {'NORMAL': 0, 'PNEUMONIA': 1}\n",
        "    df['label'] = df['label'].apply(lambda x: to_categorical(label_dict[x], num_classes=2))\n",
        "    \n",
        "    x = np.zeros((len(df.index), 224, 224, 3), dtype=np.float32)\n",
        "    y = np.zeros((len(df.index), 2), dtype=np.float32)\n",
        "    \n",
        "    for i in range(len(df.index)):\n",
        "        x[i] = df.iloc[i]['img']\n",
        "        y[i] = df.iloc[i]['label']\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN3ZbygKGpR-"
      },
      "source": [
        "# NEURAL NETWORK ALGORITHM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tm5_3IvGpR-"
      },
      "outputs": [],
      "source": [
        "def get_batches(x, y, size):\n",
        "    # shuffle order of data\n",
        "    steps = len(x)//size\n",
        "    batch_data = np.zeros((size, 224, 224, 3), dtype=np.float32)\n",
        "    batch_labels = np.zeros((size, 2), dtype=np.float32)\n",
        "    \n",
        "    indices = np.arange(len(x))\n",
        "    \n",
        "    i = 0\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        next_batch = indices[i*size:(i+1)*size]\n",
        "        for j, idx in enumerate(next_batch):\n",
        "            batch_data[j] = x[idx]\n",
        "            batch_labels[j] = y[idx]\n",
        "        i += 1\n",
        "        yield batch_data, batch_labels\n",
        "    \n",
        "    if i >= steps:\n",
        "        i=0\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTpqAuGTGpR_"
      },
      "source": [
        "## ADD CONVOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYHRpTaeGpR_"
      },
      "outputs": [],
      "source": [
        "def generate_model():\n",
        "    # Start with 3 convolutions+relu, 3 max pooling, and 2 dense layers \n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolution Layer 1: 32 filters\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))\n",
        "    # ReLu Activation\n",
        "    # transforms output values between 0 to 1\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling Layer 1\n",
        "    # downsample input representation (reduce overfitting and computational cost)\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    # Convolution Layer 2: 32 filters\n",
        "    model.add(Conv2D(32, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling Layer 2\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    # Convolution Layer 3: 64 filters\n",
        "    model.add(Conv2D(64, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling Layer 3\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    # converts 3D feature mapsa to 1D feature vectors\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense Layer 1\n",
        "    model.add(Dense(64))\n",
        "    model.add(Activation('relu'))\n",
        "    # Dense Layer 2\n",
        "    model.add(Dense(2))\n",
        "    # softmax activation\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evkySdqzGpSA"
      },
      "outputs": [],
      "source": [
        "#val_path = \"chest_xray/val\"\n",
        "#test_path = \"chest_xray/test\"\n",
        "#train_path = \"chest_xray/train\"\n",
        "#val_path = \"chest_xray/val\"\n",
        "#test_path = \"chest_xray/test\"\n",
        "train_data = load_data(train_path)\n",
        "val_data = load_data(val_path)\n",
        "test_data = load_data(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PeAmzfqZGpSA",
        "outputId": "5fd9e46c-a4be-4313-8a58-7fb1ac20c5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4dee63ff8ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-bbff2b44baa4>\u001b[0m in \u001b[0;36mdisplay_image\u001b[0;34m(data, preprocessed)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "display_image(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USE PRETRAINED NETWORK"
      ],
      "metadata": {
        "id": "JFSdoGMBFeKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Hk4Rs6GpSB"
      },
      "outputs": [],
      "source": [
        "train_x, train_y = preprocess_data(train_data)\n",
        "val_x, val_y = preprocess_data(val_data)\n",
        "test_x, test_y = preprocess_data(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stx0vG4CGpSC"
      },
      "outputs": [],
      "source": [
        "# number of images fed to the network in a single forward pass\n",
        "batch_size = 16\n",
        "# number of times network processes the whole training dataset\n",
        "epochs = 3\n",
        "# number of steps in one epoch\n",
        "training_steps = len(train_data.index)//batch_size\n",
        "# binary cross-entropy as loss function due to 2 classes, 0 and 1\n",
        "\n",
        "model = generate_model()\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer=Adam(learning_rate=1e-5), \n",
        "              metrics=['mse', 'accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etM26sT6GpSC"
      },
      "outputs": [],
      "source": [
        "batch_generator = get_batches(train_x, train_y, batch_size)\n",
        "history = model.fit(batch_generator, \n",
        "                    epochs=epochs, \n",
        "                    steps_per_epoch=training_steps,\n",
        "                    validation_data = (val_x, val_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9-Vu1h6GpSC"
      },
      "source": [
        "# TRANSFER LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUhhT8m4GpSD"
      },
      "outputs": [],
      "source": [
        "def generate_transfer_model():\n",
        "    model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "    \n",
        "    layer = Dense(1024, activation='relu')(model.layers[-4].output)\n",
        "    layer = Dropout(0.7)(layer)\n",
        "    layer = Dense(512, activation='relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(2, activation='softmax')(layer)\n",
        "    model = Model(model.input, layer)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8o1h5_aGpSD"
      },
      "outputs": [],
      "source": [
        "vgg_model = generate_transfer_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1b7AVyLGpSD"
      },
      "outputs": [],
      "source": [
        "for layer in vgg_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "vgg_model.compile(loss='binary_crossentropy',\n",
        "                  optimizer = Adam(learning_rate=1e-4, decay=1e-5),\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PEVZmwoGpSE"
      },
      "outputs": [],
      "source": [
        "history = vgg_model.fit(batch_generator, \n",
        "                        epochs=epochs,\n",
        "                        steps_per_epoch=training_steps,\n",
        "                        validation_data = (val_x, val_y),\n",
        "                        class_weight={0:1.0, 1:0.4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRnTYxMMGpSE"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, test_labels, batch_size=16)\n",
        "print(f'Manual model - Loss: {loss}\\t Acc: {acc}')\n",
        "loss, acc = vgg_model.evaluate(test_data, test_labels, batch_size=16)\n",
        "print(f'VGG model - Loss: {loss}\\t Acc: {acc}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
