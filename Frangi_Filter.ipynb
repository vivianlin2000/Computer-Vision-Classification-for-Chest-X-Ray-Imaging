{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivianlin2000/Computer-Vision-Classification-for-Chest-X-Ray-Imaging/blob/main/Frangi_Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go \n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from skimage import filters\n",
        "from skimage import exposure\n",
        "from skimage import io, color\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave, imread\n",
        "from sklearn.decomposition import FastICA\n",
        "!pip install python-gdcm\n",
        "!pip install pydicom\n",
        "import pydicom as pyd\n",
        "from PIL import Image\n",
        "# split into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#############################################################################\n",
        "study_df = pd.read_csv('./data/train_study_level.csv', index_col=None)\n",
        "image_df = pd.read_csv('./data/train_image_level.csv', index_col=None)\n",
        "#############################################################################\n",
        "X_train, X_test, y_train, y_test = train_test_split(study_df['id'], study_df[study_df.columns[1:]], test_size=0.2, random_state=42)\n",
        "print('# of train samples:', len(X_train))\n",
        "print('# of test samples:', len(X_test))\n",
        "#############################################################################\n",
        "# create image dictionary \n",
        "# {study_id: [image_ids]}\n",
        "\n",
        "study_img_dict = {}\n",
        "for study_id in study_df['id']:\n",
        "    img_ids = image_df[image_df['StudyInstanceUID'] == study_id]['id']\n",
        "    study_img_dict[study_id] = img_ids\n",
        "\n",
        "study_img_dict\n",
        "#############################################################################\n",
        "# iterate through studies\n",
        "for study_id in X_train:\n",
        "    count = 0\n",
        "    # iterate through study's images\n",
        "    for img_id in study_img_dict[study_id]:\n",
        "        # get the file\n",
        "        # check if file exists\n",
        "        if img_id not in file_dict.keys():\n",
        "            break\n",
        "        # read file\n",
        "        dicom = pyd.read_file(file_dict[img_id], stop_before_pixels=False)\n",
        "        img = dicom.pixel_array\n",
        "        # resize image for faster processing\n",
        "        x_image = resize(img, (512, 512), anti_aliasing=True)"
      ],
      "metadata": {
        "id": "mhgCejfahqAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frangi Filter Pseudocode\n"
      ],
      "metadata": {
        "id": "CH2dbYKmi-Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs/params: \n",
        "\n",
        "image : (NxM) ndarray\n",
        "- input image array\n",
        "\n",
        "sigmas : iterable of floats\n",
        "- scales of filter\n",
        "\n",
        "alpha : float\n",
        "- frangi correction constant that adjusts the filter's sensitivity to deviation from a plate-like structure\n",
        "\n",
        "beta : float\n",
        "- frangi correction constant that adjusts the filter's sensitivity to deviation from a blob-like structure\n",
        "\n",
        "c : float\n",
        "- frangi correction constant that adjusts the filter's sensitivity to areas of high variance/texture/structure\n",
        "\n",
        "black_ridges : boolean\n",
        "- True if black ridges; False if white ridges.\n",
        "\n",
        "mode : {'constant', 'reflect', 'wrap', 'nearest', 'mirror'}\n",
        "- method of handling values outside image borders\n",
        "\n",
        "cval : float\n",
        "- Used in conjunction with mode 'constant', the value outside the image boundaries."
      ],
      "metadata": {
        "id": "ThBm7ve9vVQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Frangi Filter Pseudocode:\n",
        "\n",
        "#get data/image into desirable format\n",
        "#generalizable to 3d, for our purposes we will focus on 2d\n",
        "check black ridges\n",
        "check constraints of inputs (image dimensions -- 2d or 3d array, nonnegative sigma values)\n",
        "\n",
        "#filter along all sigma\n",
        "for sigma in sigmas:\n",
        "  hessian <- hessian_matrix(image, sigma=sigma) #hessian at scale sigma\n",
        "  hessian_eigenvalues <- hessian_matrix_eigvals(hessian)\n",
        "  hessian_eigenvalues <- sort_by_abs(hessian eigenvalues)\n",
        "\n",
        "  # compute sensitivity to deviation from a plate-like structure eq11,15\n",
        "  # (3d) r_a <- lambda2/lambda3\n",
        "\n",
        "  # compute sensitivity to deviation from a blob-like structure, eq10,15\n",
        "  # (3d) r_b <- lambda1/sqrt(lambda2 * lambda3)\n",
        "  r_b <- lambda1/lambda2\n",
        "\n",
        "  # compute sensitivity to areas of high variance/texture/structure, eq12\n",
        "  # (3d) s <- sqrt(lambda1**2 + lambda2**2 + lambda3**2)\n",
        "  s <- sqrt(lambda1**2 + lambda2**2)\n",
        "\n",
        "  # output image\n",
        "  for each pixel in image:\n",
        "    filter <- (np.exp(-r_b**2/beta**2)*(1 - np.exp(-s/c**2)))\n",
        "\n",
        "# Remove background\n",
        "filtered_arr[lambdas_arr > 0] <- 0\n",
        "# Return for every pixel the maximum value over all (sigma) scales (less noisy)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "803G78cezBpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "inajEi53i758"
      }
    }
  ]
}